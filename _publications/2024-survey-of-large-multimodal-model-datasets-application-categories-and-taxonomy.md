---
layout: "single"
title: "Survey of large multimodal model datasets, application categories and taxonomy"
authors: "Pattnayak, Priyaranjan; Patel, Hitesh Laxmichand; Kumar, Bhargava; Agarwal, Amit; Banerjee, Ishan; Panda, Srikant; Kumar, Tejaswini;"
year: 2024
tags: [LLM, Retrieval, Document-AI, Vision-Language, Systems]
paper_url: "https://arxiv.org/abs/2412.17759"
pdf_url: "https://arxiv.org/pdf/2412.17759.pdf"
search: true
---

**Abstract.** Multimodal learning, a rapidly evolving field in artificial intelligence, seeks to construct more versatile and robust systems by integrating and analyzing diverse types of data, including text, images, audio, and video. Inspired by the human ability to assimilate information through many senses, this method enables applications such as text-to-video conversion, visual question answering, and image captioning. Recent developments in datasets that support multimodal language models (MLLMs) are highlighted in this overview. Large-scale multimodal datasets are essential because they allow for thorough testing and training of these models. With an emphasis on their contributions to the discipline, the study examines a variety of datasets, including those for training, domain-specific tasks, and real-world applications. It also emphasizes how crucial benchmark datasets are for assessing models' performance in a range of scenarios, scalability, and applicability. Since multimodal learning is always changing, overcoming these obstacles will help AI research and applications reach new heights.
