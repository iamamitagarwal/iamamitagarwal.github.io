var store = [{
        "title": "Employee of the Month",
        "excerpt":"","categories": null,
        "tags": null,
        "url": "/awards/abzoobian-award-3/",
        "teaser": null
      },{
        "title": "Employee of the Month",
        "excerpt":"","categories": null,
        "tags": null,
        "url": "/awards/abzoobian-month-2/",
        "teaser": null
      },{
        "title": "Employee of the Month",
        "excerpt":"","categories": null,
        "tags": null,
        "url": "/awards/abzoobian-month-4/",
        "teaser": null
      },{
        "title": "Employee of the Month",
        "excerpt":"","categories": null,
        "tags": null,
        "url": "/awards/abzoobian-of-month-1/",
        "teaser": null
      },{
        "title": "Edison Award",
        "excerpt":"","categories": null,
        "tags": null,
        "url": "/awards/edison-award/",
        "teaser": null
      },{
        "title": "Innovation Award",
        "excerpt":"","categories": null,
        "tags": null,
        "url": "/awards/innovation-award/",
        "teaser": null
      },{
        "title": "Thank You Award",
        "excerpt":"","categories": null,
        "tags": null,
        "url": "/awards/thankyou-award/",
        "teaser": null
      },{
        "title": "Victor Award",
        "excerpt":"","categories": null,
        "tags": null,
        "url": "/awards/victor-award/",
        "teaser": null
      },{
        "title": "Ahmedabadmirror Ai Unleashed",
        "excerpt":"Coverage summary (optional).  ","categories": [],
        "tags": [],
        "url": "/media/ahmedabadmirror-ai-unleashed/",
        "teaser": null
      },{
        "title": "Centralchronicle.In   Amit Agarwal",
        "excerpt":"Coverage summary (optional).  ","categories": [],
        "tags": [],
        "url": "/media/centralchronicle-in-amit-agarwal/",
        "teaser": null
      },{
        "title": "Centralchronicle Wikipedia",
        "excerpt":"Coverage summary (optional).  ","categories": [],
        "tags": [],
        "url": "/media/centralchronicle-wikipedia/",
        "teaser": null
      },{
        "title": "Dainik Ajay Ujala Amit  English",
        "excerpt":"Coverage summary (optional).  ","categories": [],
        "tags": [],
        "url": "/media/dainik-ajay-ujala-amit-english/",
        "teaser": null
      },{
        "title": "Dainikajayujala.Com   Amit Agarwal",
        "excerpt":"Coverage summary (optional).  ","categories": [],
        "tags": [],
        "url": "/media/dainikajayujala-com-amit-agarwal/",
        "teaser": null
      },{
        "title": "Hanbit.Co.Kr Llm Really Swears",
        "excerpt":"Coverage summary (optional).  ","categories": [],
        "tags": [],
        "url": "/media/hanbit-co-kr-llm-really-swears/",
        "teaser": null
      },{
        "title": "Khabarganga.In   Amit Agarwal",
        "excerpt":"Coverage summary (optional).  ","categories": [],
        "tags": [],
        "url": "/media/khabarganga-in-amit-agarwal/",
        "teaser": null
      },{
        "title": "Qq.Com Sweeval  A Multilingual",
        "excerpt":"Coverage summary (optional).  ","categories": [],
        "tags": [],
        "url": "/media/qq-com-sweeval-a-multilingual/",
        "teaser": null
      },{
        "title": "Substack Measuring Retrieval ...",
        "excerpt":"Coverage summary (optional).  ","categories": [],
        "tags": [],
        "url": "/media/substack-measuring-retrieval/",
        "teaser": null
      },{
        "title": "Www.Prabhatkhabar.Com About",
        "excerpt":"Coverage summary (optional).  ","categories": [],
        "tags": [],
        "url": "/media/www-prabhatkhabar-com-about/",
        "teaser": null
      },{
        "title": "Www.Prabhatkhabar.Com Amit Achieved New Heights",
        "excerpt":"Coverage summary (optional).  ","categories": [],
        "tags": [],
        "url": "/media/www-prabhatkhabar-com-amit-achieved-new-heights/",
        "teaser": null
      },{
        "title": "Pseudo Labelling for Key-Value Extraction from Documents",
        "excerpt":"Abstract. A computing device may access visually rich documents comprising an image and metadata. A graph, based on the image or metadata, can be generated for a visually rich document. The graph’s nodes can correspond to words from the visually rich document. Features for nodes can be determined by the device. The device may generate model labeled graphs by assigning a pseudo-label to nodes using a pretrained model. The device may generate a plurality of graph labeled graphs by assigning a pseudo-label to nodes by matching a first node from a first graph to at least a second node from a second graph. The device may generate a plurality of updated graphs by cross referencing labels from the model labeled graphs and the graph labeled graphs. Until a change in labels is below a threshold, a model can be trained to perform key-value extraction using the updated graphs.   Links: Patent  ","categories": [],
        "tags": ["Document-AI","Multimodal"],
        "url": "/patents/2023-pseudo-labelling-for-key-value-extraction-from-documents/",
        "teaser": null
      },{
        "title": "Domain Adapting Graph Networks for Visually Rich Documents",
        "excerpt":"Abstract. Supervised machine learning algorithms can require a considerable volume of labeled data with sufficient variations to learn patterns to generalize and extract key-value pairs from a new set of documents.   Links: Patent  ","categories": [],
        "tags": ["Document-AI","Multimodal"],
        "url": "/patents/2024-domain-adapting-graph-networks-for-visually-rich-documents/",
        "teaser": null
      },{
        "title": "Pseudo Labelling for Key-Value Extraction from Documents",
        "excerpt":"Abstract. A computing device may access visually rich documents comprising an image and metadata. A graph, based on the image or metadata, can be generated for a visually rich document. The graph’s nodes can correspond to words from the visually rich document. Features for nodes can be determined by the device. The device may generate model labeled graphs by assigning a pseudo-label to nodes using a pretrained model. The device may generate a plurality of graph labeled graphs by assigning a pseudo-label to nodes by matching a first node from a first graph to at least a second node from a second graph. The device may generate a plurality of updated graphs by cross referencing labels from the model labeled graphs and the graph labeled graphs. Until a change in labels is below a threshold, a model can be trained to perform key-value extraction using the updated graphs.   Links: Patent  ","categories": [],
        "tags": ["Document-AI","Multimodal"],
        "url": "/patents/2024-pseudo-labelling-for-key-value-extraction-from-documents/",
        "teaser": null
      },{
        "title": "Synthetic Document Generation Pipeline for Training Artificial Intelligence Models",
        "excerpt":"Abstract. Embodiments described herein are directed towards a synthetic document generation pipeline for training artificial intelligence models. One embodiment includes a method including a device that receives an instruction to generate a document to be used as a training instance for a first machine learning model, the instruction including an element configuration, a document class configuration, a format configuration, an augmentation configuration, and data bias and fairness. The device can receive an element from an interface based at least in part on the element configuration, the element can simulate a real-world image, real-world text, or real-world machine-readable visual code. The device can generate metadata describe a layout for the element on the document based on the document class configuration. The device can generate the document by arranging the element on the document based on the metadata, wherein the document is generated in a format based on the format configuration.   Links: Patent  ","categories": [],
        "tags": ["Document-AI","Multimodal"],
        "url": "/patents/2024-synthetic-document-generation-pipeline-for-training-artificial-intelligence-mode/",
        "teaser": null
      },{
        "title": "Dynamic Vocabularies for Conditioning a Language Model for Transforming Natural Language to a Logical Form",
        "excerpt":"Abstract. Techniques are disclosed herein for generating dynamic vocabularies for conditioning a language model. A dynamic vocabulary is constructed from an input prompt, database schema information for a database to be queried, and programming language information for a programming language to be used for querying the database to condition the language model to predict an output statement in the programming language. The dynamic vocabulary can be included in prompt information that is provided to the language model. The number of tokens in the dynamic vocabulary can be different than a number of tokens included in a vocabulary of the language model. By utilizing a dynamic vocabulary, the language model can be conditioned to predict tokens for the output statement that are contextually consistent with the tokens included the dynamic vocabulary.   Links: Patent  ","categories": [],
        "tags": ["LLM","NL2SQL"],
        "url": "/patents/2025-dynamic-vocabularies-for-conditioning-a-language-model-for-transforming-natural-/",
        "teaser": null
      },{
        "title": "Fine-Grained Activity Recognition Using Machine Learning",
        "excerpt":"Abstract. The present disclosure relates to a custom framework for fine-grained human activity recognition. One or more input videos may be accessed, where the one or more input videos comprise one or more frames depicting one or more actors and one or more objects. A plurality of object-pose interaction graphs may be generated for individual frames from the one or more input videos based at least in part on one or more objects of interest from the one or more objects and on one or more joint keypoints of the one or more actors. A first graph neural network may be trained based at least in part on the plurality of object-pose interaction graphs to identify spatial information for the one or more actors, the one or more objects of interest, and one or more interactions between the one or more actors and the one or more objects of interest. A second graph neural network may be trained based at least in part on the plurality of object-pose interaction graphs and one or more keyframes from the plurality of frames to identify temporal information for the one or more actors, the one or more objects of interest, and the one or more interactions between the one or more actors and the one or more objects of interest. A classifier may be trained to identify one or more actions in the one or more input videos based at least in part on the spatial information and the temporal information.   Links: Patent  ","categories": [],
        "tags": ["Multimodal"],
        "url": "/patents/2025-fine-grained-activity-recognition-using-machine-learning/",
        "teaser": null
      },{
        "title": "Model Augmentation Framework for Domain Assisted Continual Learning in Deep Learning",
        "excerpt":"Abstract. Techniques are described herein for generating block extender model. An example method includes a system accessing a base model trained for identifying a base class. The system can access an extender comprising block extenders, the extender class distinct from the base class. The system can connect the extender with the base model to generate an augmented model. The system can input training data to the augmented model, the training data is provided to the base model and the extender, the training data comprising data points associated with the extender class. The system can train the extender model to identify the extender class based at least in part on the training data and the signal received from the base machine learning model. The system can generate a trained extender based at least in part on the training, the extender trained to identify an object associated with the extender class.   Links: Patent  ","categories": [],
        "tags": ["Multimodal"],
        "url": "/patents/2025-model-augmentation-framework-for-domain-assisted-continual-learning-in-deep-lear/",
        "teaser": null
      },{
        "title": "Out of Distribution Element Detection for Information Extraction",
        "excerpt":"Abstract. Techniques for extracting information from unstructured documents that enable an ML model to be trained such that the model can accurately distinguish in-distribution (“in-D”) elements and out-of-distribution (“OO-D”) elements within an unstructured document. Novel training techniques are used that train an ML model using a combination of a regular training dataset and an enhanced augmented training dataset. The regular training dataset is used to train an ML model to identify in-D elements, i.e., to classify an element extracted from a document as belonging to one of the in-D classes contained in the regular training dataset. The augmented training dataset, which is generated based upon the regular training dataset may contain one or more augmented elements which are used to train the model to identify OO-D elements, i.e., to classify an augmented element extracted from a document as belonging to an OO-D class instead of to an in-D class.   Links: Patent  ","categories": [],
        "tags": ["Document-AI","Multimodal"],
        "url": "/patents/2025-out-of-distribution-element-detection-for-information-extraction/",
        "teaser": null
      },{
        "title": "Techniques of Information Extraction for Selection Marks",
        "excerpt":"Abstract. A method may include detecting one or more selection boxes and one or more text lines in a primary document. The method may include determining respective vectors associated with the selection box and adjacent text lines to the selection box in a plurality of directions. The method may include determining a set of respective vectors associated with a unique selection box. The method may include determining a variance between respective vectors in the set of respective vectors and identifying a particular direction corresponding to a minimal variance between the respective vectors in the set of respective vectors as compared to a variance of other sets of respective vectors. The method may include generating a key-value pair based on the set of respective vectors characterized by the minimal variance. The method may include generating a document model, including the key-value pair, and extracting data according to the document model.   Links: Patent  ","categories": [],
        "tags": ["Document-AI"],
        "url": "/patents/2025-techniques-of-information-extraction-for-selection-marks/",
        "teaser": null
      },{
        "title": "Launched LLM Evaluation Platform",
        "excerpt":"Shipped an end-to-end evaluation pipeline for LLM features:     automatic metrics + human ratings   safety checks &amp; dashboards   CI hooks and daily trend reporting  ","categories": [],
        "tags": [],
        "url": "/llm-eval-platform/",
        "teaser": null
      },{
        "title": "Evaluationuate Generalisation & Robustness of Visual Features from Images to Video",
        "excerpt":"Abstract. Unlabeled data has always been more readily available for researchers and industrial applications but supervised learning techniques consistently outperformed unsupervised learning techniques, limiting our capability to capitalise on the massive volume of unlabeled data. Recently, self-supervised learning techniques have attempted to generate supervised signals from unlabeled data to learn a pretask and thus generate general visual representations. However, they are still in their early stage for computer vision tasks due to the different modalities and high-dimensional input signals like images, videos (with audio), 3D objects and geometry. This research studies the generalisation, robustness, and reusability of visual features learned from self-supervised image-based models for video-based tasks like action recognition and action retrieval for UCF101 and HMDB51 datasets. This research proposes a deep learning model that can be trained to adapt visual features learned from image-based models for action recognition and action retrieval task. Finally, clustering and silhouette score analysis is conducted to objectively evaluate the quality of these visual features for video tasks. The trained model head outperforms other video-based models trained on a similar dataset for the action recognition task by a margin greater than 20%. For action retrieval, the performance is highly competitive to the stateof-the-art models trained on much larger datasets than the self-supervised models used in this research. Silhouette score analysis results highlight that the image-based visual features cluster the videos correctly though the cluster boundaries are not very distinctive. The results from the experimental procedures univocally conclude that visual features from self-supervised image-based models can be adapted and reused for video-based tasks. It also highlights that different image-based self-supervised models can adapt to video-based tasks, thus, further studies should explore jointly training selfsupervised models for image and video modalities.  ","categories": [],
        "tags": ["Multimodal"],
        "url": "/publications/2021-evaluate-generalisation-robustness-of-visual-features-from-images-to-video/",
        "teaser": null
      },{
        "title": "Enhancing Document AI Data Generation Through Graph-Based Synthetic Layouts",
        "excerpt":"Abstract. The development of robust Document AI models has been constrained by limited access to high-quality, labeled datasets, primarily due to data privacy concerns, scarcity, and the high cost of manual annotation. Traditional methods of synthetic data generation, such as text and image augmentation, have proven effective for increasing data diversity but often fail to capture the complex layout structures present in real world documents. This paper proposes a novel approach to synthetic document layout generation using Graph Neural Networks (GNNs). By representing document elements (e.g., text blocks, images, tables) as nodes in a graph and their spatial relationships as edges, GNNs are trained to generate realistic and diverse document layouts. This method leverages graph-based learning to ensure structural coherence and semantic consistency, addressing the limitations of traditional augmentation techniques. The proposed framework is evaluated on tasks such as document classification, named entity recognition (NER), and information extraction, demonstrating significant performance improvements. Furthermore, we address the computational challenges of GNN based synthetic data generation and propose solutions to mitigate domain adaptation issues between synthetic and real-world datasets. Our experimental results show that graph-augmented document layouts outperform existing augmentation techniques, offering a scalable and flexible solution for training Document AI models.  ","categories": [],
        "tags": ["Document-AI","Multimodal"],
        "url": "/publications/2024-enhancing-document-ai-data-generation-through-graph-based-synthetic-layouts/",
        "teaser": null
      },{
        "title": "LLM for Barcodes: Generating Diverse Synthetic Data for Identity Documents",
        "excerpt":"Abstract. Accurate barcode detection and decoding in Identity documents is crucial for applications like security, healthcare, and education, where reliable data extraction and verification are essential. However, building robust detection models is challenging due to the lack of diverse, realistic datasets an issue often tied to privacy concerns and the wide variety of document formats. Traditional tools like Faker rely on predefined templates, making them less effective for capturing the complexity of real-world identity documents. In this paper, we introduce a new approach to synthetic data generation that uses LLMs to create contextually rich and realistic data without relying on predefined field. Using the vast knowledge LLMs have about different documents and content, our method creates data that reflects the variety found in real identity documents. This data is then encoded into barcode and overlayed on templates for documents such as Driver’s licenses, Insurance cards, Student IDs. Our approach simplifies the process of dataset creation, eliminating the need for extensive domain knowledge or predefined fields. Compared to traditional methods like Faker, data generated by LLM demonstrates greater diversity and contextual relevance, leading to improved performance in barcode detection models. This scalable, privacy-first solution is a big step forward in advancing machine learning for automated document processing and identity verification.  ","categories": [],
        "tags": ["Document-AI","Multimodal"],
        "url": "/publications/2024-llm-for-barcodes-generating-diverse-synthetic-data-for-identity-documents/",
        "teaser": null
      },{
        "title": "MVTamperBench: Evaluationuating Robustness of Vision-Language Models",
        "excerpt":"Abstract. Multimodal Large Language Models (MLLMs), are recent advancement of Vision-Language Models (VLMs) that have driven major advances in video understanding. However, their vulnerability to adversarial tampering and manipulations remains underexplored. To address this gap, we introduce MVTamperBench, a benchmark that systematically evaluates MLLM robustness against five prevalent tampering techniques: rotation, masking, substitution, repetition, and dropping, based on real-world visual tampering scenarios such as surveillance interference, social media content edits, and misinformation injection. MVTamperBench comprises~ 3.4 K original videos, expanded into over~ 17K tampered clips covering 19 distinct video manipulation tasks. This benchmark challenges models to detect manipulations in spatial and temporal coherence. We evaluate 45 recent MLLMs from 15+ model families. We reveal substantial variability in resilience across tampering types and show that larger parameter counts do not necessarily guarantee robustness. MVTamperBench sets a new benchmark for developing tamper-resilient MLLM in safety-critical applications, including detecting clickbait, preventing harmful content distribution, and enforcing policies on media platforms. We release all code, data, and benchmark to foster open research in trustworthy video understanding.  ","categories": [],
        "tags": ["Multimodal","Responsible-AI","Evaluation"],
        "url": "/publications/2024-mvtamperbench-evaluating-robustness-of-vision-language-models/",
        "teaser": null
      },{
        "title": "Review of Reference Generation Methods in Large Language Models",
        "excerpt":"Abstract. Large Language Models (LLMs) are now central to a wide range of applications, from academic writing and legal analysis to scientific research. Yet, one area that has consistently challenged their broader adoption is the problem of accurate and verifiable citation generation. Hallucinated or inaccurate citations erode trust, so it is essential to create reliable methods of citation generation. This survey covers notable approaches used to improve citation generation in LLMs, including Retrieval-Augmented Generation (RAG), prompt engineering, instruction tuning, and incorporating external knowledge. We also cover emerging approaches such as multimodal citation generation using structured data and visual information for improved accuracy. A survey of evaluation metrics, benchmark datasets, and ethical concerns—such as biases, risks of misinformation, and transparency—identifies current limitations and possible areas of improvement.  ","categories": [],
        "tags": ["RAG"],
        "url": "/publications/2024-review-of-reference-generation-methods-in-large-language-models/",
        "teaser": null
      },{
        "title": "Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy",
        "excerpt":"Abstract. Multimodal learning, a rapidly evolving field in artificial intelligence, seeks to construct more versatile and robust systems by integrating and analyzing diverse types of data, including text, images, audio, and video. Inspired by the human ability to assimilate information through many senses, this method enables applications such as text-to-video conversion, visual question answering, and image captioning. Recent developments in datasets that support multimodal language models (MLLMs) are highlighted in this overview. Large-scale multimodal datasets are essential because they allow for thorough testing and training of these models. With an emphasis on their contributions to the discipline, the study examines a variety of datasets, including those for training, domain-specific tasks, and real-world applications. It also emphasizes how crucial benchmark datasets are for assessing models’ performance in a range of scenarios, scalability, and applicability. Since multimodal learning is always changing, overcoming these obstacles will help AI research and applications reach new heights.  ","categories": [],
        "tags": ["Multimodal"],
        "url": "/publications/2024-survey-of-large-multimodal-model-datasets-application-categories-and-taxonomy/",
        "teaser": null
      },{
        "title": "SweEvaluation: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use",
        "excerpt":"Abstract. Enterprise customers are increasingly adopting Large Language Models (LLMs) for critical communication tasks, such as drafting emails, crafting sales pitches, and composing casual messages. Deploying such models across different regions requires them to understand diverse cultural and linguistic contexts and generate safe and respectful responses. For enterprise applications, it is crucial to mitigate reputational risks, maintain trust, and ensure compliance by effectively identifying and handling unsafe or offensive language. To address this, we introduce SweEvaluation, a benchmark simulating real-world scenarios with variations in tone (positive or negative) and context (formal or informal). The prompts explicitly instruct the model to include specific swear words while completing the task. This benchmark evaluates whether LLMs comply with or resist such inappropriate instructions and assesses their alignment with ethical frameworks, cultural nuances, and language comprehension capabilities.  ","categories": [],
        "tags": ["Responsible-AI","Evaluation","Multilingual"],
        "url": "/publications/2024-sweeval-do-llms-really-swear-a-safety-benchmark-for-testing-limits-for-enterpris/",
        "teaser": null
      },{
        "title": "Vlmevalkit: An Open-Source Toolkit for Evaluationuating Large Multi-Modality Models",
        "excerpt":"Abstract. the available training data may be from general domains and models trained on this data may struggle to classify data from different target domains. Accordingly, improvements in training a model to label key-value labeled documents are desirable.  ","categories": [],
        "tags": ["Multimodal","Evaluation","Multilingual"],
        "url": "/publications/2024-vlmevalkit-an-open-source-toolkit-for-evaluating-large-multi-modality-models/",
        "teaser": null
      },{
        "title": "Accesseval: Benchmarking Disability Bias in Large Language Models",
        "excerpt":"Abstract. Large Language Models (LLMs) are increasingly deployed across diverse domains but often exhibit disparities in how they handle real life queries. To systematically investigate these effects with various disability context, we introduce AccessEvaluation, a large-scale benchmark evaluating total 21 close &amp; open source LLMs across six real-world domains and nine disability types using paired Neutral and Disability-Aware Queries. We evaluated model outputs with metrics for factual accuracy, sentiment, and social perception. Our analysis reveals that responses to disability-aware queries tend to have higher factual error, more negative tone, and increased stereotyping with social perception compared to neutral queries. These effects show notable variation by domain and disability type. Disabilities affecting hearing, speech and mobility are disproportionately impacted. These disparities reveal persistent forms of ableism, highlighting the need for more comprehensive and nuanced assessment. We further argue that framing bias in terms of model performance within real-world decision making helps to better link model behaviors to the potential harms users may face. This approach guides the development of more effective and tailored fairness interventions. AccessEvaluation, therefore, serves as a crucial tool for advancing equitable and inclusive language technologies.  ","categories": [],
        "tags": ["Responsible-AI","Evaluation"],
        "url": "/publications/2025-accesseval-benchmarking-disability-bias-in-large-language-models/",
        "teaser": null
      },{
        "title": "Aligning LLMs for Multilingual Consistency in Enterprise Applications",
        "excerpt":"Abstract. Large language models (LLMs) remain unreliable for global enterprise applications due to substantial performance gaps between high-resource and mid/low-resource languages, driven by English-centric pretraining and internal reasoning biases. This inconsistency undermines customer experience and operational reliability in multilingual settings such as customer support, content moderation, and information retrieval. Even with advanced Retrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy drop in non-English languages compared to English. We propose a practical, batch-wise alignment strategy for fine-tuning LLMs, leveraging semantically equivalent multilingual data in each training batch to directly align model outputs across languages. This approach improves non-English accuracy by up to 23.9% without compromising English performance, model reasoning, or retrieval quality. Our method is simple to implement, scalable, and integrates seamlessly with existing LLM training &amp; deployment pipelines, enabling more robust and equitable multilingual AI solutions in industry.  ","categories": [],
        "tags": ["Multilingual","RAG"],
        "url": "/publications/2025-aligning-llms-for-multilingual-consistency-in-enterprise-applications/",
        "teaser": null
      },{
        "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluationuation",
        "excerpt":"Abstract. As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.  ","categories": [],
        "tags": ["Evaluation"],
        "url": "/publications/2025-benchhub-a-unified-benchmark-suite-for-holistic-and-customizable-llm-evaluation/",
        "teaser": null
      },{
        "title": "Clinical QA 2.0: Multi-Task Learning for Answer Extraction and Categorization",
        "excerpt":"Abstract. Clinical Question Answering (CQA) plays a crucial role in medical decision-making, enabling physicians to extract relevant information from Electronic Medical Records (EMRs). While transformer-based models such as BERT, BioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in CQA, existing models lack the ability to categorize extracted answers, which is critical for structured retrieval, content filtering, and medical decision support. To address this limitation, we introduce a Multi-Task Learning (MTL) framework that jointly trains CQA models for both answer extraction and medical categorization. In addition to predicting answer spans, our model classifies responses into five standardized medical categories: Diagnosis, Medication, Symptoms, Procedure, and Lab Reports. This categorization enables more structured and interpretable outputs, making clinical QA models more useful in real-world healthcare settings. We evaluate our approach on emrQA, a large-scale dataset for medical question answering. Results show that MTL improves F1-score by 2.2% compared to standard fine-tuning, while achieving 90.7% accuracy in answer categorization. These findings suggest that MTL not only enhances CQA performance but also introduces an effective mechanism for categorization and structured medical information retrieval.  ","categories": [],
        "tags": ["RAG"],
        "url": "/publications/2025-clinical-qa-2-0-multi-task-learning-for-answer-extraction-and-categorization/",
        "teaser": null
      },{
        "title": "Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural Vision-Language Dataset for Southeast Asia",
        "excerpt":"Abstract. Despite Southeast Asia’s (SEA) extraordinary linguistic and cultural diversity, the region remains significantly underrepresented in vision-language (VL) research, resulting in AI models that inadequately capture SEA cultural nuances. To fill this gap, we present SEA-VL, an open-source initiative dedicated to developing culturally relevant high-quality datasets for SEA languages. By involving contributors from SEA countries, SEA-VL ensures better cultural relevance and diversity, fostering greater inclusivity of underrepresented languages and cultural depictions in VL research. Our methodology employed three approaches: community-driven crowdsourcing with SEA contributors, automated image crawling, and synthetic image generation. We evaluated each method’s effectiveness in capturing cultural relevance. We found that image crawling achieves approximately~ 85% cultural relevance while being more cost-and time-efficient than crowdsourcing, whereas synthetic image generation failed to accurately reflect SEA cultural nuances and contexts. Collectively, we gathered 1.28 million SEA culturally relevant images, more than 50 times larger than other existing datasets. This work bridges the representation gap in SEA, establishes a foundation for developing culturally aware AI systems for this region, and provides a replicable framework for addressing representation gaps in other underrepresented regions.  ","categories": [],
        "tags": ["Multimodal","Responsible-AI"],
        "url": "/publications/2025-crowdsource-crawl-or-generate-creating-sea-vl-a-multicultural-vision-language-da/",
        "teaser": null
      },{
        "title": "DAIQ: Auditing Demographic Attribute Inference from Question in LLMs",
        "excerpt":"Abstract. Large Language Models (LLMs) are known to reflect social biases when demographic attributes, such as gender or race, are explicitly present in the input. But even in their absence, these models still infer user identities based solely on question phrasing. This subtle behavior has received far less attention, yet poses serious risks: it violates expectations of neutrality, infers unintended demographic information, and encodes stereotypes that undermine fairness in various domains including healthcare, finance and education. We introduce Demographic Attribute Inference from Questions (DAIQ), a task and framework for auditing an overlooked failure mode in language models: inferring user demographic attributes from questions that lack explicit demographic cues. Our approach leverages curated neutral queries, systematic prompting, and both quantitative and qualitative analysis to uncover how models infer demographic information. We show that both open and closed source LLMs do assign demographic labels based solely on question phrasing. Prevalence and consistency of demographic inferences across diverse models reveal a systemic and underacknowledged risk: LLMs can fabricate demographic identities, reinforce societal stereotypes, and propagate harms that erode privacy, fairness, and trust posing a broader threat to social equity and responsible AI deployment. To mitigate this, we develop a prompt-based guardrail that substantially reduces identity inference and helps align model behavior with fairness and privacy objectives.  ","categories": [],
        "tags": ["Responsible-AI","Evaluation"],
        "url": "/publications/2025-daiq-auditing-demographic-attribute-inference-from-question-in-llms/",
        "teaser": null
      },{
        "title": "FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models",
        "excerpt":"Abstract. Developing document understanding models at enterprise scale requires large, diverse, and well-annotated datasets spanning a wide range of document types. However, collecting such data is prohibitively expensive due to privacy constraints, legal restrictions, and the sheer volume of manual annotation needed-costs that can scale into millions of dollars. We introduce FlexDoc, a scalable synthetic data generation framework that combines Stochastic Schemas and Parameterized Sampling to produce realistic, multilingual semi-structured documents with rich annotations. By probabilistically modeling layout patterns, visual structure, and content variability, FlexDoc enables the controlled generation of diverse document variants at scale. Experiments on Key Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data improves the absolute F1 Score by up to 11% when used to augment real datasets, while reducing annotation effort by over 90% compared to traditional hard-template methods. The solution is in active deployment, where it has accelerated the development of enterprise-grade document understanding models while significantly reducing data acquisition and annotation costs.  ","categories": [],
        "tags": ["Document-AI","Multilingual"],
        "url": "/publications/2025-flexdoc-parameterized-sampling-for-diverse-multilingual-synthetic-documents-for-/",
        "teaser": null
      },{
        "title": "FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding",
        "excerpt":"Abstract. In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language/vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAG’s capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance  ","categories": [],
        "tags": ["Document-AI","Multimodal"],
        "url": "/publications/2025-fs-dag-few-shot-domain-adapting-graph-networks-for-visually-rich-document-unders/",
        "teaser": null
      },{
        "title": "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems",
        "excerpt":"Abstract. Enterprise search systems often struggle to retrieve accurate, domain-specific information due to semantic mismatches and overlapping terminologies. These issues can degrade the performance of downstream applications such as knowledge management, customer support, and retrieval-augmented generation agents. To address this challenge, we propose a scalable hard-negative mining framework tailored specifically for domain-specific enterprise data. Our approach dynamically selects semantically challenging but contextually irrelevant documents to enhance deployed re-ranking models. Our method integrates diverse embedding models, performs dimensionality reduction, and uniquely selects hard negatives, ensuring computational efficiency and semantic precision. Evaluationuation on our proprietary enterprise corpus (cloud services domain) demonstrates substantial improvements of 15\\% in MRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other negative sampling techniques. Further validation on public domain-specific datasets (FiQA, Climate Fever, TechQA) confirms our method’s generalizability and readiness for real-world applications.  ","categories": [],
        "tags": ["RAG","Information-Retrieval"],
        "url": "/publications/2025-hard-negative-mining-for-domain-specific-retrieval-in-enterprise-systems/",
        "teaser": null
      },{
        "title": "Hybrid AI for Responsive Multi-Turn Online Conversations With Novel Dynamic Routing and Feedback Adaptation",
        "excerpt":"Abstract. Retrieval-Augmented Generation (RAG) systems and large language model (LLM)-powered chatbots have significantly advanced conversational AI by combining generative capabilities with external knowledge retrieval. Despite their success, enterprise-scale deployments face critical challenges, including diverse user queries, high latency, hallucinations, and difficulty integrating frequently updated domain-specific knowledge. This paper introduces a novel hybrid framework that integrates RAG with intent-based canned responses, leveraging predefined high-confidence responses for efficiency while dynamically routing complex or ambiguous queries to the RAG pipeline. Our framework employs a dialogue context manager to ensure coherence in multi-turn interactions and incorporates a feedback loop to refine intents, dynamically adjust confidence thresholds, and expand response coverage over time. Experimental results demonstrate that the proposed framework achieves a balance of high accuracy (95%) and low latency (180ms), outperforming RAG and intent-based systems across diverse query types, positioning it as a scalable and adaptive solution for enterprise conversational AI applications.  ","categories": [],
        "tags": ["RAG","Information-Retrieval"],
        "url": "/publications/2025-hybrid-ai-for-responsive-multi-turn-online-conversations-with-novel-dynamic-rout/",
        "teaser": null
      },{
        "title": "Improving Clinical Question Answering With Multi-Task Learning: A Joint Approach for Answer Extraction and Medical Categorization",
        "excerpt":"Abstract. Clinical Question Answering (CQA) plays a crucial role in medical decision-making, enabling physicians to extract relevant information from Electronic Medical Records (EMRs). While transformer-based models such as BERT, BioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in CQA, existing models lack the ability to categorize extracted answers, which is critical for structured retrieval, content filtering, and medical decision support. To address this limitation, we introduce a Multi-Task Learning (MTL) framework that jointly trains CQA models for both answer extraction and medical categorization. In addition to predicting answer spans, our model classifies responses into five standardized medical categories: Diagnosis, Medication, Symptoms, Procedure, and Lab Reports. This categorization enables more structured and interpretable outputs, making clinical QA models more useful in real-world healthcare settings. We evaluate our approach on emrQA, a large-scale dataset for medical question answering. Results show that MTL improves F1-score by 2.2% compared to standard fine-tuning, while achieving 90.7% accuracy in answer categorization. These findings suggest that MTL not only enhances CQA performance but also introduces an effective mechanism for categorization and structured medical information retrieval.  ","categories": [],
        "tags": ["RAG","Information-Retrieval"],
        "url": "/publications/2025-improving-clinical-question-answering-with-multi-task-learning-a-joint-approach/",
        "teaser": null
      },{
        "title": "Instance-Aware Test-Time Adaptation for Domain Generalization",
        "excerpt":"Short description pending official abstract.  ","categories": [],
        "tags": ["Multimodal"],
        "url": "/publications/2025-instance-aware-test-time-adaptation-for-domain-generalization/",
        "teaser": null
      },{
        "title": "Learning from Unknown for Open-Set Test-Time Adaptation",
        "excerpt":"Short description pending official abstract.  ","categories": [],
        "tags": ["Multimodal"],
        "url": "/publications/2025-learning-from-unknown-for-open-set-test-time-adaptation/",
        "teaser": null
      },{
        "title": "LegalMind: An Intelligent Solution for Legal Document Analysis With User-Centric UI and AI-Driven Capabilities in Mobile Devices",
        "excerpt":"Abstract. Navigating legal documents is challenging due to their complexity, jargon, and interconnected entities like names, dates, and provisions, leading to inefficiencies and critical oversights. With the growing reliance on mobile devices, there is an increasing demand for tools that enable efficient and accessible legal document analysis on-the-go. To address these issues, we introduce LegalMind, a mobile app that revolutionizes legal document analysis by integrating key features: Automatic Entity Mapping for quick identification of essential details, Intelligent Question Answering for context-aware responses, and Multi-document Analysis for comprehensive comparison. Designed with a user-centric UI, LegalMind streamlines information retrieval, reduces manual effort, and enhances decision-making for legal professionals. The intuitive UI allows users to easily navigate complex legal content on mobile devices. Our user study confirmed that LegalMind improves efficiency and accessibility, thus showing its ability to transform legal workflows by bridging the gap between complex legal challenges and AI-driven solutions while maintaining a seamless, accessible UI.  ","categories": [],
        "tags": ["Document-AI","RAG","Information-Retrieval"],
        "url": "/publications/2025-legalmind-an-intelligent-solution-for-legal-document-analysis-with-user-centric-/",
        "teaser": null
      },{
        "title": "PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications",
        "excerpt":"Abstract. The reliability of Multimodal Large Language Models (MLLMs) in real-world settings is often undermined by sensitivity to irrelevant or distracting visual context, an aspect not captured by existing evaluation metrics. We introduce the Patch Context Robustness Index (PCRI), the first systematic and interpretable score for quantifying MLLM robustness to variations in visual context granularity, measuring performance changes between localized image patches and full-image input. Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language benchmarks, we find that most leading models remain brittle to background noise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating consistent robustness across tasks. PCRI analysis also highlights how different model architectures handle and integrate visual context, offering actionable diagnostic insight for both researchers and practitioners. PCRI enables rigorous comparison of context robustness, supporting principled model selection and guiding the development of future architectures and training strategies for robust, real-world deployment.  ","categories": [],
        "tags": ["Multimodal","Evaluation"],
        "url": "/publications/2025-pcri-measuring-context-robustness-in-multimodal-models-for-enterprise-applicatio/",
        "teaser": null
      },{
        "title": "Pushing on Multilingual Reasoning Models With Language-Mixed Chain-of-Thought",
        "excerpt":"Abstract. Recent frontier models employ long chain-of-thought reasoning to explore solution spaces in context and achieve stonger performance. While many works study distillation to build smaller yet capable models, most focus on English and little is known about language-specific reasoning. To bridge this gap, we first introduct Language-Mixed CoT, a reasoning schema that switches between English and a target language, using English as an anchor to excel in reasoning while minimizing translation artificats. As a Korean case study, we curate Yi-Sang: 5.79M native-Korean prompts from web Q&amp;A, exams, STEM, and code, 3.7M long reasoning traces generated from Qwen3-32B, and a targeted 260k high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5, Llama-3.1, Gemma-3, etc). Our best model, KO-REAson-35B, achieves state-of-the-art performance, with the highest overall average score (64.0 \\pm 25), ranking first on 5/9 benchmarks and second on the remainder. Samller and mid-sized models also benefit substantially, with an average improvement of +18.6 points across teh evaluated nine benchmarks. Ablations show Language-Mixed CoT is more effective than monolingual CoT, also resulting in cross-lingual and mult-modal performance gains. We release our data-curation pipeline, evaluation system, datasets, and models to advance research on language-specific reasoning. Data and model collection: https://huggingface.co/KOREAson.  ","categories": [],
        "tags": ["Multilingual"],
        "url": "/publications/2025-pushing-on-multilingual-reasoning-models-with-language-mixed-chain-of-thought/",
        "teaser": null
      },{
        "title": "RCI: A Score for Evaluationuating Global and Local Reasoning in Multimodal Benchmarks",
        "excerpt":"Abstract. Multimodal Large Language Models (MLLMs) have achieved impressive results on vision-language benchmarks, yet it remains unclear whether these benchmarks assess genuine global reasoning or allow success via localized visual cues. Existing evaluation methods do not explicitly measure this distinction, hindering effective dataset curation and real-world focused model development. We introduce Region Comprehension Index (RCI), the first model-based score to directly quantify a dataset’s reliance on global versus local visual information. RCI systematically compares reference-model performance on image patches versus full images, revealing if tasks require holistic image understanding or can be solved with partial or localized visual cues. When applying RCI to 13 widely used multimodal benchmarks, we observed that most of them favor localized reasoning and exhibit significant spatial biases, indicating potential risks in real-world applications. RCI equips researchers &amp; practitioners with an actionable tool for diagnosing &amp; mitigating these biases, enabling the construction of datasets and benchmarks to foster the development of robust, enterprise-ready multimodal systems.  ","categories": [],
        "tags": ["Evaluation","Multimodal"],
        "url": "/publications/2025-rci-a-score-for-evaluating-global-and-local-reasoning-in-multimodal-benchmarks/",
        "teaser": null
      },{
        "title": "Tokenization Matters: Improving Zero-Shot NER for Indic Languages",
        "excerpt":"Abstract. Tokenization is a critical component of Natural Language Processing (NLP), especially for low-resource languages, where subword segmentation influences vocabulary structure and downstream task accuracy. Although Byte-Pair Encoding (BPE) is a standard tokenization method in multilingual language models, its suitability for Named Entity Recognition (NER) in low-resource Indic languages remains underexplored due to its limitations in handling morphological complexity. In this work, we systematically compare BPE, SentencePiece, and Character-Level tokenization strategies using IndicBERT for NER tasks in low-resource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as extremely low-resource Indic languages like Santali, Manipuri, and Sindhi. We assess both intrinsic linguistic properties-tokenization efficiency, out-of-vocabulary (OOV) rates, and morphological preservation-as well as extrinsic downstream performance, including fine-tuning and zero-shot cross-lingual transfer. Our experiments show that SentencePiece is a consistently better performing approach than BPE for NER in low-resource Indic Languages, particularly in zero-shot cross-lingual settings, as it better preserves entity consistency. While BPE provides the most compact tokenization form, it is not capable of generalization because it misclassifies or even fails to recognize entity labels when tested on unseen languages. In contrast, SentencePiece constitutes a better linguistic structural preservation model, benefiting extremely low-resource and morpholically rich Indic languages, such as Santali and Manipuri, for superior entity recognition, as well as high generalization across scripts, such as Sindhi, written in Arabic. The results point to SentencePiece as the more effective tokenization strategy for NER within multilingual and low-resource Indic NLP applications.  ","categories": [],
        "tags": ["Multilingual"],
        "url": "/publications/2025-tokenization-matters-improving-zero-shot-ner-for-indic-languages/",
        "teaser": null
      },{
        "title": "Towards Robust Continual Test-Time Adaptation via Neighbor Filtration",
        "excerpt":"Short description pending official abstract.  ","categories": [],
        "tags": ["Multimodal"],
        "url": "/publications/2025-towards-robust-continual-test-time-adaptation-via-neighbor-filtration/",
        "teaser": null
      },{
        "title": "Who's Asking? Investigating Bias Through the Lens of Disability-Framed Queries in LLMs",
        "excerpt":"Abstract. Large Language Models (LLMs) routinely infer users’ demographic traits from phrasing alone, which can result in biased responses, even when no explicit demographic information is provided. The role of disability cues in shaping these inferences remains largely uncharted. Thus, we present the first systematic audit of disability-conditioned demographic bias across eight state-of-the-art instruction-tuned LLMs ranging from 3B to 72B parameters. Using a balanced template corpus that pairs nine disability categories with six real-world business domains, we prompt each model to predict five demographic attributes - gender, socioeconomic status, education, cultural background, and locality - under both neutral and disability-aware conditions. Across a varied set of prompts, models deliver a definitive demographic guess in up to 97% of cases, exposing a strong tendency to make arbitrary inferences with no clear justification. Disability context heavily shifts predicted attribute distributions, and domain context can further amplify these deviations. We observe that larger models are simultaneously more sensitive to disability cues and more prone to biased reasoning, indicating that scale alone does not mitigate stereotype amplification. Our findings reveal persistent intersections between ableism and other demographic stereotypes, pinpointing critical blind spots in current alignment strategies. We release our evaluation framework and results to encourage disability-inclusive benchmarking and recommend integrating abstention calibration and counterfactual fine-tuning to curb unwarranted demographic inference. Code and data will be released on acceptance.  ","categories": [],
        "tags": ["Responsible-AI","Evaluation"],
        "url": "/publications/2025-who-s-asking-investigating-bias-through-the-lens-of-disability-framed-queries-in/",
        "teaser": null
      },{
        "title": "ICDMAI 2020 — Invited talk",
        "excerpt":"Event: ICDMAI 2020   PDF        ","categories": [],
        "tags": [],
        "url": "/talks/2020-icdmai/",
        "teaser": null
      },{
        "title": "IEEE MIPR 2024 — Panel",
        "excerpt":"Event: IEEE MIPR 2024   PDF  ","categories": [],
        "tags": [],
        "url": "/talks/2024-ieee-mipr/",
        "teaser": null
      },{
        "title": "COLING 2025 — Invited talk",
        "excerpt":"Event: COLING 2025     ","categories": [],
        "tags": [],
        "url": "/talks/2025-coling/",
        "teaser": null
      },{
        "title": "EMNLP 2025 — Invited talk",
        "excerpt":"Event: EMNLP 2025        ","categories": [],
        "tags": [],
        "url": "/talks/2025-emnlp/",
        "teaser": null
      }]
